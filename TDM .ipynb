{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b236a281",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/abudb/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import nltk\n",
    "import re\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "# Initialize spacy ‘en’ model, keeping only component needed for lemmatization and creating an engine:\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=['parser', 'ner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ba33a5a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Article ID</th>\n",
       "      <th>Article Title</th>\n",
       "      <th>Date</th>\n",
       "      <th>Source</th>\n",
       "      <th>Source type</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Company list</th>\n",
       "      <th>Country list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2373425154.xml</td>\n",
       "      <td>Women's bank helps Ivorian children to swap la...</td>\n",
       "      <td>2020-03-08</td>\n",
       "      <td>AFP International Text Wire in English</td>\n",
       "      <td>tba</td>\n",
       "      <td>0.041</td>\n",
       "      <td>[]</td>\n",
       "      <td>[('Ghana', 1)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>217312819.xml</td>\n",
       "      <td>Take My Brother, Please</td>\n",
       "      <td>2008-05-11</td>\n",
       "      <td>New York Times Book Review</td>\n",
       "      <td>tba</td>\n",
       "      <td>0.041</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>318308262.xml</td>\n",
       "      <td>Masters or fakes?</td>\n",
       "      <td>2000-06-08</td>\n",
       "      <td>The Times</td>\n",
       "      <td>tba</td>\n",
       "      <td>-0.474</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>375318621.xml</td>\n",
       "      <td>Child labour, \"almost nonexistent\" in Algeria</td>\n",
       "      <td>2010-06-12</td>\n",
       "      <td>Algeria Press Service</td>\n",
       "      <td>tba</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>[]</td>\n",
       "      <td>[('Algeria', 2)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>269287394.xml</td>\n",
       "      <td>Good conscience worth billions</td>\n",
       "      <td>2000-12-19</td>\n",
       "      <td>The Province</td>\n",
       "      <td>tba</td>\n",
       "      <td>0.078</td>\n",
       "      <td>[]</td>\n",
       "      <td>[('Canada', 2)]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Article ID                                      Article Title  \\\n",
       "0  2373425154.xml  Women's bank helps Ivorian children to swap la...   \n",
       "1   217312819.xml                            Take My Brother, Please   \n",
       "2   318308262.xml                                  Masters or fakes?   \n",
       "3   375318621.xml      Child labour, \"almost nonexistent\" in Algeria   \n",
       "4   269287394.xml                     Good conscience worth billions   \n",
       "\n",
       "         Date                                  Source Source type  Sentiment  \\\n",
       "0  2020-03-08  AFP International Text Wire in English         tba      0.041   \n",
       "1  2008-05-11              New York Times Book Review         tba      0.041   \n",
       "2  2000-06-08                               The Times         tba     -0.474   \n",
       "3  2010-06-12                   Algeria Press Service         tba     -0.010   \n",
       "4  2000-12-19                            The Province         tba      0.078   \n",
       "\n",
       "  Company list      Country list  \n",
       "0           []    [('Ghana', 1)]  \n",
       "1           []                []  \n",
       "2           []                []  \n",
       "3           []  [('Algeria', 2)]  \n",
       "4           []   [('Canada', 2)]  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"/Users/abudb/Coding/hace hackathon/convert_to_dataframe.csv\"\n",
    "df = pd.read_csv (path, index_col=0, skipinitialspace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7720eec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = df.iloc[0][\"Article Title\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "546053df",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "stopwords_lower = [s.lower() for s in stopwords]\n",
    "\n",
    "\n",
    "def text_preprocessing(str_input): \n",
    "     #tokenization, remove punctuation, lemmatization\n",
    "     words=[token.lemma_ for token in nlp(str_input) if not token.is_punct]\n",
    " \n",
    "     # remove symbols, websites, email addresses \n",
    "     words = [re.sub(r\"[^A-Za-z@]\", \"\", word) for word in words] \n",
    "     words = [re.sub(r\"\\S+com\", \"\", word) for word in words]\n",
    "     words = [re.sub(r\"\\S+@\\S+\", \"\", word) for word in words] \n",
    "     words = [word for word in words if word!=' ']\n",
    "     words = [word for word in words if len(word)!=0] \n",
    " \n",
    "     #remove stopwords     \n",
    "     words=[word.lower() for word in words if word.lower() not in     stopwords_lower]\n",
    "     #combine a list into one string   \n",
    "     string = \" \".join(words)\n",
    "     return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de2ffd39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'woman bank help ivorian child swap labour class'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_preprocessing(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60e88d9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'burundian child kidnappers jailed'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "index = random.randint(0, df.shape[0])\n",
    "text_preprocessing(df.iloc[index][\"Article Title\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5db0d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"cleaned_article_title\"]=df[\"Article Title\"].apply(text_preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c04f86fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'today': 4, 'sunny': 3, 'day': 0, 'happy': 2, 'friday': 1, 'wonderful': 5}\n",
      "(2, 6)\n",
      "[[0.44943642 0.         0.         0.6316672  0.6316672  0.        ]\n",
      " [0.37997836 0.53404633 0.53404633 0.         0.         0.53404633]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "text = [\"Today is a sunny day\", \"Happy friday Wonderful day\"]\n",
    "\n",
    "vectorizer =TfidfVectorizer(stop_words='english')\n",
    "# vectorizer.fit(text)\n",
    "# vector = vectroizer.transform(text)\n",
    "vector = vectorizer.fit_transform(text)\n",
    "\n",
    "print(vectorizer.vocabulary_)\n",
    "print(vector.shape)\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea395e19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "8c15ef8637308dd522c01dadfa4691cede6a96e1e3afd3c753dcdec800276c0a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
